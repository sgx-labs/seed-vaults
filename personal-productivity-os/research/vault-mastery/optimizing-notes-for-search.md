---
title: "Optimizing Notes for SAME Search"
tags: [vault-mastery, optimization, search, embeddings, writing]
content_type: research
domain: productivity
workstream: vault-mastery
---

# Optimizing Notes for SAME Search

SAME finds your notes using vector embeddings — 768-dimensional "meaning fingerprints" generated by an AI model. How you write directly affects how well your notes are found. These seven principles are grounded in how embedding models and SAME's ranking algorithm actually work.

## 1. Titles Matter Most

SAME's ranking algorithm computes a bidirectional overlap score between your search query and the note's title. Notes with strong title matches are promoted into a "high tier" that outranks even closer vector matches. A title like "Weekly Review Framework for ADHD Brains" matches the query "adhd weekly review" on three terms. A title like "Notes" matches nothing.

**Do this:**
- Use specific, descriptive titles that contain the words someone would search for
- Include the core concept and the domain: "Context Switching Cost in Knowledge Work"
- Avoid generic titles: "Ideas," "Notes," "Thoughts," "Misc"

**Example:**
- Bad: `Stuff I Learned`
- Good: `Spaced Repetition for Long-Term Retention`

## 2. YAML Frontmatter Is Indexed

Tags in your frontmatter create additional match surface for filtered searches. SAME supports filtering by `domain`, `workstream`, and `tags`. A note with `tags: [adhd, task-initiation, strategies]` will be found by searches filtered to any of those terms, even if the body text does not mention them explicitly.

Use 3-5 specific tags per note. Mix broad categories with specific terms. See `research/vault-mastery/frontmatter-guide.md` for the full guide.

## 3. The First Paragraph Is the Embedding Anchor

Embedding models weight tokens by position. The first 100-200 tokens carry disproportionate influence on the resulting vector. SAME prepends the title to the text before embedding (`title + "\n" + chunk.Text`), so your title is always part of the embedding. But the first paragraph of body text is the next most influential segment.

**Do this:**
- State the core concept in your first 2-3 sentences
- Do not start with meta-commentary ("In this note I will explore...")
- Lead with the insight, not the setup

**Example:**
- Bad first paragraph: "I have been thinking about this topic for a while and wanted to write down some thoughts."
- Good first paragraph: "Context switching costs 23 minutes of recovery time per interruption. This makes multitasking one of the most expensive habits in knowledge work."

## 4. One Concept Per Note

An embedding vector represents the "meaning" of a chunk of text as a single point in 768-dimensional space. When a note covers one topic, that point is precise — it sits close to related queries and far from unrelated ones. When a note covers three topics, the vector is an average of all three meanings. It matches everything weakly and nothing strongly.

**Do this:**
- If a note covers two distinct concepts, split it into two notes
- Use hub notes to link related concepts (see `research/vault-mastery/note-architecture.md`)
- Ask: "If someone searched for this note's topic, would they find only what they need?"

## 5. Keep Notes Under 200 Lines

SAME chunks notes at H2 heading boundaries. Each chunk gets its own embedding vector. Shorter notes produce fewer, more precise chunks. A 50-line note with two H2 sections produces two tight embeddings. A 500-line note with ten sections produces ten embeddings, some of which may be noisy.

Additionally, chunks that exceed 7,500 characters are split further at paragraph boundaries, which can break semantic coherence. Keeping notes focused and concise avoids this.

**The sweet spot:** 40-150 lines, 2-4 H2 sections.

## 6. Use Domain-Specific Vocabulary

Embedding models match on semantic similarity, not keyword overlap. But semantic similarity is built from the words you use. If your note uses "getting things done" but you search for "GTD," the match may be weak because the model treats them as partially related but not identical.

**Do this:**
- Use the terms someone would actually search for
- Include both the acronym and the full phrase: "GTD (Getting Things Done)"
- Use precise vocabulary: "task initiation" instead of "starting things"
- Mirror the language of your own queries

## 7. Path Structure Matters

SAME includes directory path components in title overlap scoring. A note at `research/adhd/hyperfocus.md` gets path-based overlap for queries containing "adhd" or "hyperfocus." A note at `notes/misc/thing.md` gets no useful path signal.

**Do this:**
- Use descriptive directory names: `research/energy/`, not `notes/stuff/`
- Match directory names to the domains you search for
- Keep paths 2-3 levels deep: `category/topic/specific-note.md`

## The Quality Chain

Good notes produce good embeddings. Good embeddings produce good search results. Good search results make the agent more useful. The agent being more useful means you use it more, which means more notes get captured. This is the compound loop that makes SAME valuable over time.

See: `research/vault-mastery/embedding-science.md`, `research/vault-mastery/frontmatter-guide.md`
